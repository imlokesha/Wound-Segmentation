{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-11T04:28:57.247008Z","iopub.status.busy":"2023-10-11T04:28:57.246655Z","iopub.status.idle":"2023-10-11T04:29:06.691390Z","shell.execute_reply":"2023-10-11T04:29:06.689798Z","shell.execute_reply.started":"2023-10-11T04:28:57.246978Z"},"id":"-AUz8eKzsDRU","outputId":"a1d53d1c-2f11-483f-d937-5a40698f090a","trusted":true},"outputs":[],"source":["%pip install -q --upgrade datasets transformers evaluate peft"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:06.694664Z","iopub.status.busy":"2023-10-11T04:29:06.694229Z","iopub.status.idle":"2023-10-11T04:29:06.699010Z","shell.execute_reply":"2023-10-11T04:29:06.698026Z","shell.execute_reply.started":"2023-10-11T04:29:06.694619Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()\n","# hf_MZI****************************"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:06.701092Z","iopub.status.busy":"2023-10-11T04:29:06.700492Z","iopub.status.idle":"2023-10-11T04:29:06.712450Z","shell.execute_reply":"2023-10-11T04:29:06.711418Z","shell.execute_reply.started":"2023-10-11T04:29:06.701062Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","\n","# from huggingface_hub import cached_download, hf_hub_url\n","\n","id2label = {0:\"background\", 1:\"wound\"}\n","label2id = {v: k for k, v in id2label.items()}\n","num_labels = len(id2label)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:06.716270Z","iopub.status.busy":"2023-10-11T04:29:06.715724Z","iopub.status.idle":"2023-10-11T04:29:15.859631Z","shell.execute_reply":"2023-10-11T04:29:15.858141Z","shell.execute_reply.started":"2023-10-11T04:29:06.716236Z"},"trusted":true},"outputs":[],"source":["from tensorflow import keras\n","\n","keras.backend.set_image_data_format('channels_last')\n","# %pip install graphviz\n","import os\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import tensorflow as tf\n","import graphviz\n","import keras\n","import datetime\n","from torch import nn\n","import torch\n","\n","# Data\n","from keras import backend as K\n","from tqdm import tqdm\n","\n","print(os.getcwd())\n","\n","# Data Viz\n","import matplotlib.pyplot as plt\n","import gc\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Model\n","from keras.models import Model\n","from keras.layers import Layer\n","from keras.layers import Conv2D\n","from keras.layers import Dropout\n","from keras.layers import UpSampling2D\n","from keras.layers import concatenate\n","from keras.layers import Add\n","from keras.layers import Multiply\n","from keras.layers import Input\n","from keras.layers import MaxPool2D\n","from keras.layers import BatchNormalization\n","from keras.layers import Flatten\n","from keras.layers import Dense\n","# Callbacks\n","from keras.callbacks import Callback\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","\n","# %pip install -qU albumentations[imgaug]\n","# %pip install -qU imgaug\n","# import albumentations as A\n","import tensorflow_addons as tfa\n","\n","%pip install -qU tf_explain\n","from tf_explain.core.grad_cam import GradCAM\n","\n","import cv2\n","from keras.metrics import MeanIoU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.864675Z","iopub.status.busy":"2023-10-11T04:29:15.863783Z","iopub.status.idle":"2023-10-11T04:29:15.872533Z","shell.execute_reply":"2023-10-11T04:29:15.871096Z","shell.execute_reply.started":"2023-10-11T04:29:15.864616Z"},"id":"6mfZB3NVHIAm","outputId":"bcc9c6c2-1197-41fd-d9f4-6cd21604d474","trusted":true},"outputs":[],"source":["# df = pd.read_csv(\"/kaggle/input/wounds/description.csv\")\n","# df[\"Path to files\"] = df[\"Path to files\"].apply(lambda x: os.path.join(dir,x).replace(\"\\\\\",\"/\"))\n","# df.head()\n","\n","# i = df['Path to files'][0]\n","# image = tf.keras.utils.load_img(os.path.join(i,'photo.jpg'))\n","# input_arr = tf.keras.utils.img_to_array(image)\n","# input_arr\n","\n","# img_shape = (1224, 816)\n","\n","# images = np.zeros((len(df), img_shape[0], img_shape[1], 3))\n","# masks = np.zeros((len(df), img_shape[0], img_shape[1], 1))\n","\n","# for idx,i in tqdm(enumerate(df['Path to files'])):\n","#   image = tf.keras.utils.load_img(os.path.join(i,'photo.jpg'), target_size=img_shape,  method=\"lanczos\")\n","#   input_arr = tf.keras.utils.img_to_array(image) / 255.\n","#   images[idx] = input_arr\n","#   image = tf.keras.utils.load_img(os.path.join(i,'mask.png'), color_mode=\"grayscale\" , target_size=img_shape, method=\"lanczos\")\n","#   input_arr = tf.keras.utils.img_to_array(image) / 255.\n","#   masks[idx] = input_arr\n","\n","# os.path.join(dir,\"images.npy\")\n","\n","# with open(os.path.join(dir,\"images.npy\"), 'wb') as f:\n","#   np.save(f,images)\n","\n","# with open(os.path.join(dir,\"masks.npy\"), 'wb') as f:\n","#   np.save(f,masks)\n","\n","# masks = np.load(os.path.join(dir,\"masks.npy\"))\n","# images = np.load(os.path.join(dir,\"images.npy\"))\n","\n","\n","# images = np.load(\"/kaggle/input/wounds/images.npy\")\n","# masks = np.load(\"/kaggle/input/wounds/masks.npy\")\n","\n","# train_idx, test_idx = train_test_split(range(len(df)), test_size=0.1, random_state=50)\n","\n","# train_dataset = tf.data.Dataset.from_tensor_slices((images[train_idx],masks[train_idx]))\n","# test_dataset  = tf.data.Dataset.from_tensor_slices((images[test_idx],  masks[test_idx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.875392Z","iopub.status.busy":"2023-10-11T04:29:15.874189Z","iopub.status.idle":"2023-10-11T04:29:15.891722Z","shell.execute_reply":"2023-10-11T04:29:15.890523Z","shell.execute_reply.started":"2023-10-11T04:29:15.875328Z"},"trusted":true},"outputs":[],"source":["img_shape = [128,128,3]\n","# img_shape = [160,160,3]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.897848Z","iopub.status.busy":"2023-10-11T04:29:15.896919Z","iopub.status.idle":"2023-10-11T04:29:15.914532Z","shell.execute_reply":"2023-10-11T04:29:15.910071Z","shell.execute_reply.started":"2023-10-11T04:29:15.897813Z"},"trusted":true},"outputs":[],"source":["# def brightness_contrast(image):\n","#     image = tf.image.random_brightness(image, 0.3)\n","#     image = tf.image.random_contrast(image, 0.7, 1.3)\n","#     return image\n","    \n","# def rotate(image, mask):\n","#     angle1 = tf.random.uniform(shape=[])*45\n","#     image1 = tfa.image.rotate(image, np.pi/360*angle1)\n","#     mask1  = tfa.image.rotate(mask, np.pi/360*angle1)\n","    \n","#     angle2 = tf.random.uniform(shape=[])*-45\n","#     image2 = tfa.image.rotate(image, np.pi/360*angle2)\n","#     mask2  = tfa.image.rotate(mask, np.pi/360*angle2)\n","    \n","# #     angle3 = tf.random.uniform(shape=[])*45\n","# #     image3 = tfa.image.rotate(image, np.pi/360*angle3)\n","# #     mask3  = tfa.image.rotate(mask, np.pi/360*angle3)\n","    \n","    \n","# #     angle4 = tf.random.uniform(shape=[])*-45\n","# #     image4 = tfa.image.rotate(image, np.pi/360*angle4)\n","# #     mask4  = tfa.image.rotate(mask, np.pi/360*angle4)\n","    \n","    \n","    \n","# #     return [image1, image2, image3, image4], [mask1, mask2, mask3, mask4]\n","\n","#     return [image1, image2], [mask1, mask2]\n","    \n","# def centre_crop(image, mask):\n","#     amount = tf.random.uniform(shape=[])*0.6+0.4\n","#     image  = tf.image.central_crop(image, amount)\n","#     mask   = tf.image.central_crop(mask, amount)\n","#     return image, mask\n","    \n","# def resize(image, mask): \n","#     image = tf.image.resize(image, img_shape[:2], method = \"lanczos3\")\n","#     mask  = tf.image.resize( mask, img_shape[:2], method = \"lanczos3\") \n","#     image =  (image - np.min(image)) / (np.max(image)-np.min(image))\n","#     if np.max(mask)>np.min(mask):\n","#         mask =  (mask - np.min(mask)) / (np.max(mask)-np.min(mask))\n","#         mask = tf.cast(mask>0.5, tf.float32)\n","#     else:\n","#         mask = np.zeros_like(mask)\n","\n","#     return image, mask\n","\n","# def aug(image, mask, augment = True):\n","#     if augment == True:\n","#         image1 = image\n","#         mask1 = mask\n","\n","#         image2 = tf.image.flip_left_right(image)\n","#         mask2  = tf.image.flip_left_right(mask)\n","\n","#         image3 = tf.image.flip_up_down(image)\n","#         mask3  = tf.image.flip_up_down(mask)\n","\n","#         image4 = tf.image.flip_left_right(image)\n","#         mask4  = tf.image.flip_left_right(mask)\n","#         image4 = tf.image.flip_up_down(image4)\n","#         mask4  = tf.image.flip_up_down(mask4)\n","\n","#         images = [image1, image2, image3, image4]\n","#         masks = [mask1, mask2, mask3, mask4]\n","        \n","        \n","#         for i in range(4):\n","#             demoimage, demomask = rotate(images[i], masks[i])\n","#             images += demoimage\n","#             masks += demomask\n","        \n","#         for i in range(len(images)):\n","#             images[i] = brightness_contrast(images[i])\n","#             images[i], masks[i] = centre_crop(images[i], masks[i])\n","#             images[i], masks[i] = resize(images[i], masks[i])\n","    \n","#     images = images\n","#     masks = masks\n","#     return {'images':images, 'masks':masks}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.922186Z","iopub.status.busy":"2023-10-11T04:29:15.920356Z","iopub.status.idle":"2023-10-11T04:29:15.934668Z","shell.execute_reply":"2023-10-11T04:29:15.932835Z","shell.execute_reply.started":"2023-10-11T04:29:15.922146Z"},"trusted":true},"outputs":[],"source":["# def get_data(path, labels_present=True, adjust_labels = False, augment = True, change_mask = False):\n","#     size = len(os.listdir(os.path.join(path,\"images\")))\n","    \n","#     images = []\n","#     if labels_present:\n","#         masks = []\n","\n","#     for idx,i in enumerate(tqdm(os.listdir(os.path.join(path,\"images\")))):\n","#         image = tf.keras.utils.load_img(os.path.join(path,\"images\",i))\n","#         image = tf.keras.utils.img_to_array(image)\n","#         y_nonzero, x_nonzero, _ = np.nonzero(image)\n","#         image = image[np.min(y_nonzero):np.max(y_nonzero), np.min(x_nonzero):np.max(x_nonzero)]\n","#         if labels_present:\n","#             mask = tf.keras.utils.load_img(os.path.join(path,\"labels\",i), color_mode=\"grayscale\")\n","#             mask = tf.keras.utils.img_to_array(mask)\n","#             mask = mask[np.min(y_nonzero):np.max(y_nonzero), np.min(x_nonzero):np.max(x_nonzero)]\n","            \n","#             if change_mask==True:\n","#                 mask=mask*255\n","            \n","#             if augment:\n","#                 sample = aug(image=image, mask=mask)                    \n","#                 masks+= sample['masks']\n","#                 images+= sample['images']\n","#             else:\n","#                 image = tf.image.resize(image, img_shape[:2], method = \"lanczos3\")\n","#                 mask  = tf.image.resize( mask, img_shape[:2], method = \"lanczos3\") \n","#                 image =  (image - np.min(image)) / (np.max(image)-np.min(image))\n","#                 if np.max(mask)>np.min(mask):\n","# #                     mask =  (mask - np.min(mask)) / (np.max(mask)-np.min(mask))\n","#                     mask = tf.cast(mask>0.5, tf.float32)\n","#                 else:\n","#                     mask = np.zeros_like(mask)\n","#                 images += [image]\n","#                 masks += [mask]\n","                \n","#         else:\n","#             image = tf.image.resize(image, img_shape[:2], method = \"lanczos3\") \n","#             image =  (image - np.min(image)) / (np.max(image)-np.min(image))\n","#             images+=[image]\n","        \n","#     if labels_present:\n","#         return (np.array(images),np.array(masks))\n","#     else:\n","#         return np.array(images)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.937794Z","iopub.status.busy":"2023-10-11T04:29:15.936245Z","iopub.status.idle":"2023-10-11T04:29:15.949263Z","shell.execute_reply":"2023-10-11T04:29:15.948064Z","shell.execute_reply.started":"2023-10-11T04:29:15.937750Z"},"trusted":true},"outputs":[],"source":["# train_images, train_masks = get_data(\"/kaggle/input/wounds/Foot Ulcer Segmentation Challenge/train\")\n","# validation_images, validation_masks = get_data(\"/kaggle/input/wounds/Foot Ulcer Segmentation Challenge/validation\", augment = False)\n","\n","# test_images1 = get_data(\"/kaggle/input/wounds/Foot Ulcer Segmentation Challenge/test\", labels_present = False)\n","\n","# train_images, train_masks = get_data(\"/kaggle/input/wounds/Medetec_foot_ulcer_224/train\", change_mask=True)\n","# validation_images, validation_masks = get_data(\"/kaggle/input/wounds/Medetec_foot_ulcer_224/test\", augment = False, change_mask=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.960623Z","iopub.status.busy":"2023-10-11T04:29:15.958458Z","iopub.status.idle":"2023-10-11T04:29:15.965644Z","shell.execute_reply":"2023-10-11T04:29:15.964443Z","shell.execute_reply.started":"2023-10-11T04:29:15.959407Z"},"trusted":true},"outputs":[],"source":["# train_images2, train_masks2 = get_data(\"/kaggle/input/wounds/azh_wound_care_center_dataset_patches/train\")\n","# validation_images2, validation_masks2 = get_data(\"/kaggle/input/wounds/azh_wound_care_center_dataset_patches/test\", augment = False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.968394Z","iopub.status.busy":"2023-10-11T04:29:15.967644Z","iopub.status.idle":"2023-10-11T04:29:15.976259Z","shell.execute_reply":"2023-10-11T04:29:15.975169Z","shell.execute_reply.started":"2023-10-11T04:29:15.968323Z"},"trusted":true},"outputs":[],"source":["# train_images = np.concatenate([train_images, train_images2])\n","# # train_masks = np.concatenate([train_masks, train_masks2])\n","\n","# validation_images = np.concatenate([validation_images, validation_images2])\n","# validation_masks = np.concatenate([validation_masks, validation_masks2])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:15.978984Z","iopub.status.busy":"2023-10-11T04:29:15.977974Z","iopub.status.idle":"2023-10-11T04:29:16.310889Z","shell.execute_reply":"2023-10-11T04:29:16.309796Z","shell.execute_reply.started":"2023-10-11T04:29:15.978950Z"},"trusted":true},"outputs":[],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:16.313575Z","iopub.status.busy":"2023-10-11T04:29:16.312753Z","iopub.status.idle":"2023-10-11T04:29:16.320903Z","shell.execute_reply":"2023-10-11T04:29:16.319818Z","shell.execute_reply.started":"2023-10-11T04:29:16.313540Z"},"trusted":true},"outputs":[],"source":["# pth = \"WoundSegOld\"\n","# os.mkdir(f\"{pth}\")\n","# os.mkdir(f\"{pth}/train\")\n","# os.mkdir(f\"{pth}/validation\")\n","# os.mkdir(f\"{pth}/train/images\")\n","# os.mkdir(f\"{pth}/train/masks\")\n","# os.mkdir(f\"{pth}/validation/images\")\n","# os.mkdir(f\"{pth}/validation/masks\")\n","\n","# for idx, i in enumerate(tqdm(np.arange(len(train_images)))):\n","#     tf.keras.utils.save_img(f\"{pth}/train/images/{idx}.png\", train_images[i], scale=True, data_format=\"channels_last\")\n","#     tf.keras.utils.save_img(f\"{pth}/train/masks/{idx}.png\", train_masks[i], scale=True, data_format=\"channels_last\")\n","    \n","\n","# for idx, i in enumerate(tqdm(np.arange(len(validation_images)))):\n","#     tf.keras.utils.save_img(f\"{pth}/validation/images/{idx}.png\", validation_images[i], scale=True, data_format=\"channels_last\")\n","#     tf.keras.utils.save_img(f\"{pth}/validation/masks/{idx}.png\", validation_masks[i], scale=True, data_format=\"channels_last\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:16.323516Z","iopub.status.busy":"2023-10-11T04:29:16.322516Z","iopub.status.idle":"2023-10-11T04:29:16.331904Z","shell.execute_reply":"2023-10-11T04:29:16.330736Z","shell.execute_reply.started":"2023-10-11T04:29:16.323481Z"},"trusted":true},"outputs":[],"source":["# idx = np.arange(len(train_images))\n","# np.random.shuffle(idx)\n","# train_images = train_images[idx]\n","# train_masks = train_masks[idx]\n","\n","# idx = np.arange(len(validation_images))\n","# np.random.shuffle(idx)\n","# validation_images = validation_images[idx]\n","# validaiton_masks = validation_masks[idx]\n","\n","# # del train_images1, train_images2, train_images3, train_masks1, train_masks2, train_masks3, validation_images1, validation_images2, validation_images3, validation_masks1, validation_masks2, validation_masks3\n","# gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:29:16.333662Z","iopub.status.busy":"2023-10-11T04:29:16.333258Z","iopub.status.idle":"2023-10-11T04:29:16.345239Z","shell.execute_reply":"2023-10-11T04:29:16.344126Z","shell.execute_reply.started":"2023-10-11T04:29:16.333627Z"},"trusted":true},"outputs":[],"source":["def plot_results(results):\n","    loss,dice_coef,precision,recall,val_loss,val_dice_coef,val_precision,val_recall=results.history.values()\n","    plt.figure(figsize=(20,10))\n","    plt.style.use(\"ggplot\")\n","\n","    plt.subplot(2,2,1)\n","    plt.title(\"Loss\")\n","    plt.plot(loss, label=\"Training\")\n","    plt.plot(val_loss, label=\"Validation\")\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.subplot(2,2,2)\n","    plt.title(\"Dice Coef\")\n","    plt.plot(dice_coef, label=\"Training\")\n","    plt.plot(val_dice_coef, label=\"Validation\")\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.subplot(2,2,3)\n","    plt.title(\"Precision\")\n","    plt.plot(precision, label=\"Training\")\n","    plt.plot(val_precision, label=\"Validation\")\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.subplot(2,2,4)\n","    plt.title(\"Recall\")\n","    plt.plot(recall, label=\"Training\")\n","    plt.plot(val_recall, label=\"Validation\")\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:16.352605Z","iopub.status.busy":"2023-10-11T04:30:16.352222Z","iopub.status.idle":"2023-10-11T04:30:16.521313Z","shell.execute_reply":"2023-10-11T04:30:16.520416Z","shell.execute_reply.started":"2023-10-11T04:30:16.352575Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoImageProcessor\n","checkpoint = \"nvidia/mit-b0\"\n","# checkpoint = \"microsoft/beit-base-finetuned-ade-640-640\"\n","image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:19.754557Z","iopub.status.busy":"2023-10-11T04:30:19.753484Z","iopub.status.idle":"2023-10-11T04:30:19.760125Z","shell.execute_reply":"2023-10-11T04:30:19.759203Z","shell.execute_reply.started":"2023-10-11T04:30:19.754527Z"},"id":"trzO5qd5sDRb","trusted":true},"outputs":[],"source":["def show_image(image, title=None):\n","    plt.imshow(image)\n","    if title is not None:\n","        plt.title(title)\n","    plt.axis('off')\n","\n","def show_mask(image, mask, cmap=None, alpha=0.3,title=None):\n","    plt.imshow(image)\n","    if title is not None:\n","        plt.title(title)\n","    plt.imshow(tf.squeeze(mask), cmap=cmap, alpha=alpha)\n","    plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:19.910554Z","iopub.status.busy":"2023-10-11T04:30:19.910162Z","iopub.status.idle":"2023-10-11T04:30:19.915156Z","shell.execute_reply":"2023-10-11T04:30:19.913951Z","shell.execute_reply.started":"2023-10-11T04:30:19.910524Z"},"trusted":true},"outputs":[],"source":["# plt.figure(figsize=(20,13))\n","# for i in range(60):\n","#     plt.subplot(5,12,i+1)\n","#     id = np.random.randint(len(train_images))\n","#     show_mask(train_images[i], train_masks[i], cmap='copper',alpha=0.5) # binary afmhot copper\n","# #     show_image(train_images[i]) \n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ToSj6RQesDR_"},"source":["# **Loss Functions**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:20.266917Z","iopub.status.busy":"2023-10-11T04:30:20.266198Z","iopub.status.idle":"2023-10-11T04:30:20.276398Z","shell.execute_reply":"2023-10-11T04:30:20.275413Z","shell.execute_reply.started":"2023-10-11T04:30:20.266879Z"},"id":"gDEVmcSFsDR_","trusted":true},"outputs":[],"source":["def dice_coef(y_true, y_pred, smooth=1):\n","    y_true_f = np.ravel(y_true)\n","    y_pred_f = np.ravel(y_pred)\n","    intersection = np.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) +smooth)\n","\n","def dice_coef_loss(y_true, y_pred):\n","    return 1 - dice_coef(y_true, y_pred)\n","\n","def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n","    y_true_pos = K.flatten(y_true)\n","    y_pred_pos = K.flatten(y_pred)\n","    true_pos = K.sum(y_true_pos * y_pred_pos)\n","    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n","    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n","    return (true_pos + smooth) / (true_pos + alpha * false_neg +(1 - alpha) * false_pos + smooth)\n","\n","def tversky_loss(y_true, y_pred):\n","    return 1 - tversky(y_true, y_pred)\n","\n","\n","def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n","    tv = tversky(y_true, y_pred)\n","    return K.pow((1 - tv), gamma)\n","\n","\n","def jacard_coef(y_true, y_pred):\n","    y_true_f = np.ravel(y_true)\n","    y_pred_f = np.ravel(y_pred)\n","    intersection = np.sum(y_true_f * y_pred_f)\n","    IoU=(intersection + 1.0) / (np.sum(y_true_f) + np.sum(y_pred_f) - intersection + 1.0)\n","    return IoU\n","\n","def jacard_coef_loss(y_true, y_pred):\n","    IoU = jacard_coef(y_true, y_pred)\n","    return -IoU"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:20.675980Z","iopub.status.busy":"2023-10-11T04:30:20.675654Z","iopub.status.idle":"2023-10-11T04:30:22.000932Z","shell.execute_reply":"2023-10-11T04:30:21.999854Z","shell.execute_reply.started":"2023-10-11T04:30:20.675956Z"},"trusted":true},"outputs":[],"source":["import evaluate\n","\n","def compute_metrics(eval_pred):\n","    with torch.no_grad():\n","        logits, labels = eval_pred\n","        logits_tensor = torch.from_numpy(logits)\n","        logits_tensor = nn.functional.interpolate(\n","            logits_tensor,\n","            size=labels.shape[-2:],\n","            mode=\"bilinear\",\n","            align_corners=False,\n","        ).argmax(dim=1)\n","\n","        pred_labels = logits_tensor.detach().cpu().numpy()\n","\n","        data_p = np.round(tf.keras.metrics.Precision()(labels, pred_labels),3)\n","        data_r = np.round(tf.keras.metrics.Recall()(labels, pred_labels),3)\n","        data_dice = np.round(dice_coef(labels, pred_labels),3)\n","        data_iou = np.round(jacard_coef(labels, pred_labels),3)\n","        image_p=[]\n","        image_r=[]\n","        image_dice=[]\n","        image_iou=[]\n","        for i in range(len(labels)):\n","            image_p.append(tf.keras.metrics.Precision()(labels[i], pred_labels[i]))\n","            image_r.append(tf.keras.metrics.Recall()(labels[i], pred_labels[i]))\n","            image_dice.append(dice_coef(labels[i], pred_labels[i]))\n","            image_iou.append(jacard_coef(labels[i], pred_labels[i]))\n","        image_p = np.round(np.mean(image_p),2)\n","        image_r = np.round(np.mean(image_r),2)\n","        image_dice = np.round(np.mean(image_dice),2)\n","        image_iou = np.round(np.mean(image_iou),2)\n","\n","\n","#         model.save_pretrained(f\"{checkpoint}/{str(datetime.datetime.now())}\")\n","\n","        return {\"data_dice\":data_dice,\"data_iou\": data_iou , \"data_p\":data_p, \"data_r\":data_r,\n","               \"image_dice\":image_dice,\"image_iou\": image_iou , \"image_p\":image_p, \"data_r\":image_r}\n","\n","\n","\n","# def compute_metrics(eval_pred):\n","#     with torch.no_grad():\n","#         logits, labels = eval_pred\n","#         logits_tensor = torch.from_numpy(logits)\n","#         logits_tensor = nn.functional.interpolate(\n","#             logits_tensor,\n","#             size=labels.shape[-2:],\n","#             mode=\"bilinear\",\n","#             align_corners=False,\n","#         ).argmax(dim=1)\n","\n","#         pred_labels = logits_tensor.detach().cpu().numpy()\n","#         metrics = metric.compute(\n","#             predictions=pred_labels,\n","#             references=labels,\n","#             num_labels=num_labels,\n","#             ignore_index=255,\n","#             reduce_labels=False,\n","#         )\n","#         for key, value in metrics.items():\n","#             if type(value) is np.ndarray:\n","#                 metrics[key] = value.tolist()\n","#         return metrics\n","#   metrics = metric.compute(\n","#         predictions=pred_labels,\n","#         references=labels,\n","#         num_labels=num_labels,\n","#         ignore_index=0,\n","#         reduce_labels=image_processor.do_reduce_labels,\n","#     )\n","\n","#     per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n","#     per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n","\n","#     metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n","#     metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n","    \n","#     return {\"val_\" + k: v for k, v in metrics.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:22.003991Z","iopub.status.busy":"2023-10-11T04:30:22.003207Z","iopub.status.idle":"2023-10-11T04:30:22.033848Z","shell.execute_reply":"2023-10-11T04:30:22.032675Z","shell.execute_reply.started":"2023-10-11T04:30:22.003958Z"},"trusted":true},"outputs":[],"source":["BATCH = 16\n","\n","\n","from datasets import Dataset, IterableDataset\n","pathx = \"/kaggle/input/wounds/FUSEGnew\"\n","\n","# # pathx = \"WoundSegOld\"\n","# def train_series():\n","#     path = f\"{pathx}/train/\"\n","#     for i in os.listdir(path+\"images\"):\n","#         image = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"images/\"+i)) \n","# #         image = tf.transpose(image, (2, 0, 1))\n","#         mask = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"masks/\"+i, color_mode=\"grayscale\"))\n","#         mask = np.squeeze(mask)\n","#         mask[mask==255] = 1       \n","#         ans= image_processor([image],[mask])\n","#         ans = {key:value[0] for key,value in ans.items()}\n","#         yield ans\n","# #         yield tf.convert_to_tensor(image),tf.convert_to_tensor(mask)\n","\n","# def validation_series():\n","#     path = f\"{pathx}/validation/\"\n","#     for i in os.listdir(path+\"images\"):\n","#         image = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"images/\"+i)) \n","# #         image = tf.transpose(image, (2, 0, 1))\n","#         mask = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"masks/\"+i, color_mode=\"grayscale\")) \n","#         mask = np.squeeze(mask)\n","#         mask[mask==255] = 1\n","        \n","#         ans= image_processor([image],[mask])\n","#         ans = {key:value[0] for key,value in ans.items()}\n","#         yield ans\n","#         yield image_processor([image],[mask])\n","#         yield tf.convert_to_tensor(image),tf.convert_to_tensor(mask)\n","\n","# train_gen = tf.data.Dataset.from_generator(\n","#     train_series, \n","#     output_signature={'labels': tf.TensorSpec(shape=(512, 512), dtype=tf.int64, name=None), 'pixel_values': tf.TensorSpec(shape=(3, 512, 512), dtype=tf.float32, name=None)}\n","#     ).batch(BATCH).prefetch(1)\n","# # 16200\n","\n","# val_gen = tf.data.Dataset.from_generator(\n","#     validation_series, \n","#     output_signature={'labels': tf.TensorSpec(shape=(512, 512), dtype=tf.int64, name=None), 'pixel_values': tf.TensorSpec(shape=(3, 512, 512), dtype=tf.float32, name=None)}\n","# ).batch(BATCH).prefetch(1)\n","# #200\n","\n","\n","\n","from torch.utils.data import DataLoader\n","# , Dataset, IterableDataset\n","# class MyDataSet(IterableDataset):\n","#     def __init__(self, pathx, kind):\n","#         self.pathx = pathx\n","#         self.kind = kind\n","        \n","#     def parse(self):\n","#         path = f\"{self.pathx}/{self.kind}/\"\n","#         for i in os.listdir(path+\"images\"):\n","#             image = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"images/\"+i)) \n","#     #         image = tf.transpose(image, (2, 0, 1))\n","#             mask = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"masks/\"+i, color_mode=\"grayscale\")) \n","#             mask = np.squeeze(mask)\n","#             mask[mask==255] = 1\n","\n","#             ans= image_processor([image],[mask])\n","#             ans = {key:value[0] for key,value in ans.items()}\n","#             yield ans\n","#     def get_stream(self):\n","#         return cycle(self.parse())\n","\n","#     def __iter__(self):\n","#         return self.get_strem()\n","    \n","#     def __len__(self):\n","#         return len(os.listdir(os.path.join(self.pathx,self.kind,\"images\")))\n","\n","# train_gen = MyDataSet(pathx, \"train\")\n","# val_gen = MyDataSet(pathx, \"validation\")\n","\n","\n","def parse(pathx, kind):\n","        path = f\"{pathx}/{kind}/\"\n","        for i in os.listdir(path+\"images\"):\n","            image = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"images/\"+i)) \n","    #         image = tf.transpose(image, (2, 0, 1))\n","            mask = tf.keras.utils.img_to_array(tf.keras.utils.load_img(path+\"masks/\"+i, color_mode=\"grayscale\")) \n","            mask = np.squeeze(mask)\n","            mask[mask==255] = 1\n","\n","            ans= image_processor([image],[mask])\n","            ans = {key:value[0] for key,value in ans.items()}\n","            yield ans\n","    \n","\n","train_gen = IterableDataset.from_generator(parse, gen_kwargs={\"pathx\":pathx, \"kind\":\"train\"})\n","val_gen = IterableDataset.from_generator(parse, gen_kwargs={\"pathx\":pathx, \"kind\":\"validation\"})\n","\n","# train_gen = DataLoader(train_gen.with_format(\"torch\"), num_workers=2) \n","# val_gen = DataLoader(val_gen.with_format(\"torch\"), num_workers=2)  \n","\n","train_gen = train_gen.with_format(\"torch\")\n","val_gen = val_gen.with_format(\"torch\")"]},{"cell_type":"markdown","metadata":{"id":"nRQpDad5sDSB"},"source":["# **Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:22.036152Z","iopub.status.busy":"2023-10-11T04:30:22.035576Z","iopub.status.idle":"2023-10-11T04:30:22.367690Z","shell.execute_reply":"2023-10-11T04:30:22.366616Z","shell.execute_reply.started":"2023-10-11T04:30:22.036119Z"},"trusted":true},"outputs":[],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:39.151293Z","iopub.status.busy":"2023-10-11T04:30:39.150956Z","iopub.status.idle":"2023-10-11T04:30:39.310277Z","shell.execute_reply":"2023-10-11T04:30:39.309409Z","shell.execute_reply.started":"2023-10-11T04:30:39.151267Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n","from transformers import BeitForSemanticSegmentation\n","model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:41.383522Z","iopub.status.busy":"2023-10-11T04:30:41.383136Z","iopub.status.idle":"2023-10-11T04:30:41.389719Z","shell.execute_reply":"2023-10-11T04:30:41.388393Z","shell.execute_reply.started":"2023-10-11T04:30:41.383494Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:41.628048Z","iopub.status.busy":"2023-10-11T04:30:41.627305Z","iopub.status.idle":"2023-10-11T04:30:41.673552Z","shell.execute_reply":"2023-10-11T04:30:41.672436Z","shell.execute_reply.started":"2023-10-11T04:30:41.628016Z"},"trusted":true},"outputs":[],"source":["from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n","peft_config = LoraConfig(target_modules=[\"query\",\"key\",\"value\"], inference_mode=False, r=8, lora_alpha=8, lora_dropout=0.1, bias=\"all\")\n","\n","model = get_peft_model(model, peft_config)\n","\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T04:30:43.796903Z","iopub.status.busy":"2023-10-11T04:30:43.796211Z"},"id":"-jx1-05usDSB","outputId":"b3cdba4f-54b5-424d-fd4b-8859065c5776","trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    report_to=\"none\",\n","    output_dir=\"segformer-b0-scene-parse-150\",\n","    learning_rate=6e-5,\n","    num_train_epochs=10,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    \n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    save_total_limit=3,\n","    remove_unused_columns=False,\n","    max_steps = len(os.listdir(os.path.join(pathx,\"train\",\"images\"))),\n","    label_names=[\"labels\"],\n","    fp16 = True\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_gen,\n","    eval_dataset=val_gen,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(model.count_params())\n","np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_variables])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save_pretrained(\"BestSegFormer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !zip -r -q MODEL.zip BestModel.h5\n","# from IPython.display import FileLink\n","# FileLink(r'MODEL.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model2 = sm.FPN(backbone_name=backbones[7],input_shape=img_shape, classes=1, activation='sigmoid')\n","\n","# l2 = tf.keras.regularizers.l2(1e-5)\n","# for layer in model.layers:\n","#     if hasattr(layer, 'kernel'):\n","# #     if isinstance(layer, tf.keras.layers.Conv2D):\n","#         model.add_loss(lambda layer=layer: l2(layer.kernel))\n","\n","\n","\n","# Compile\n","model2.compile(\n","    loss=dice_coef_loss,\n","    optimizer=tf.optimizers.Adam(learning_rate=0.00001),\n","    metrics=[dice_coef,tf.keras.metrics.Precision(),tf.keras.metrics.Recall()],\n",")\n","\n","model2.load_weights(\"BestModel.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model2.evaluate(eval_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["demo = model.predict(train_images[:10])\n","\n","plt.figure(figsize=(20,13))\n","for i in range(10):\n","    plt.subplot(5,8,i+1)\n","    id = np.random.randint(len(train_images))\n","    show_mask(train_images[i], demo[i]>0.5, cmap='binary',alpha=0.5) # binary afmhot copper\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvXLtitUsDSC","trusted":true},"outputs":[],"source":["model.save_weights(\"Segmentation.h5\")"]},{"cell_type":"markdown","metadata":{"id":"F7bnQhu4sDSC"},"source":["# **Evaluation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYBpdKxPsDSE","trusted":true},"outputs":[],"source":["# loss,segmentation_loss,prediction_loss,segmentation_accuracy,segmentation_IoU,prediction_accuracy,\\\n","# val_loss,val_segmentation_loss,val_prediction_loss,val_segmentation_accuracy,val_segmentation_IoU,val_prediction_accuracy=\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVN0Z7S4sDSF","outputId":"3f8b49b2-a64a-4122-9265-be610ed43ecf","trusted":true},"outputs":[],"source":["plt.figure(figsize=(18,12))\n","n=0\n","for i in range(1,(4*6)+1):\n","    plt.subplot(4,6,i)\n","    if n==0:\n","        id = np.random.randint(len(images))\n","        image = images[id]\n","        mask = masks[id]\n","        pred_mask = model.predict(image[np.newaxis,...])\n","        show_mask(image, mask,title=\"Original Mask: \"+labels_name[np.argmax(labels[id])],alpha=0.6, cmap='copper')\n","        n+=1\n","    elif n==1:\n","        show_mask(image, tf.cast(pred_mask>0.5,tf.float32), title=\"Predicted Mask:\",alpha=0.6,cmap='copper')\n","        n+=1\n","\n","    elif n==2:\n","        id = np.random.randint(len(images))\n","        image = images[id]\n","        mask = masks[id]\n","        pred_mask = model.predict(image[np.newaxis,...])\n","        show_mask(image, mask,title=\"Original Mask: \"+labels_name[np.argmax(labels[id])],alpha=0.6, cmap='copper')\n","        n+=1\n","    elif n==3:\n","        show_mask(image, tf.cast(pred_mask>0.5,tf.float32), title=\"Predicted Mask:\",alpha=0.6,cmap='copper')\n","        n+=1\n","\n","    elif n==4:\n","        id = np.random.randint(len(images))\n","        image = images[id]\n","        mask = masks[id]\n","        pred_mask = model.predict(image[np.newaxis,...])\n","        show_mask(image, mask,title=\"Original Mask: \"+labels_name[np.argmax(labels[id])],alpha=0.6, cmap='copper')\n","        n+=1\n","    else:\n","        show_mask(image, tf.cast(pred_mask>0.5,tf.float32), title=\"Predicted Mask:\",alpha=0.6,cmap='copper')\n","        n=0\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
